# Gerenciamento Inteligente de Carregamento de Frotas de VEs (ADP)

Este reposit√≥rio cont√©m a implementa√ß√£o em Python de uma pol√≠tica de **Programa√ß√£o Din√¢mica Aproximada (ADP)** para otimiza√ß√£o de carregamento de frotas de ve√≠culos el√©tricos (VEs).

O projeto reproduz os resultados e a metodologia proposta no artigo cient√≠fico *"Electric vehicle fleet charging management: An approximate dynamic programming policy"* (Mahyari & Freeman, 2025), demonstrando como a Intelig√™ncia Artificial pode reduzir custos operacionais e auxiliar na sustentabilidade da rede el√©trica.

## üéØ Objetivo do Projeto

O objetivo √© resolver o problema de agendamento de carga em um ambiente incerto (chegadas aleat√≥rias, pre√ßos vari√°veis), respondendo √† pergunta:

> *"Quando e quanto devo carregar cada ve√≠culo para minimizar o custo total, evitar hor√°rios de pico e garantir que todos saiam com a bateria cheia?"*

## üõ†Ô∏è Arquitetura e Tecnologias

O sistema opera em um ciclo de **Aprendizado por Refor√ßo** cont√≠nuo:

1.  **Ambiente:** Simula√ß√£o de 24 horas com chegadas estoc√°sticas de VEs.
2.  **Solver (O C√©rebro Matem√°tico):** Um modelo de Programa√ß√£o Linear Inteira Mista (MILP) constru√≠do com **Pyomo** e resolvido via **GLPK**. Ele toma decis√µes a cada 15 minutos.
3.  **ADP (O Aprendizado):** Usa **Regress√£o Linear** para aprender o valor futuro das decis√µes atuais, ajustando pesos ($\zeta$) para caracter√≠sticas do sistema (ex: carga restante, urg√™ncia).

### Estrutura de Arquivos

  * `main.py`: Loop principal de treinamento, simula√ß√£o e gera√ß√£o de gr√°ficos. Possui sistema de *checkpoint* para treino cont√≠nuo.
  * `solver.py`: Implementa√ß√£o do modelo matem√°tico MILP (restri√ß√µes f√≠sicas e el√©tricas).
  * `features.py`: Extra√ß√£o de caracter√≠sticas do estado (State Features) normalizadas.
  * `heuristics.py`: Algoritmos de compara√ß√£o (Benchmarks) baseados em regras (EDLD/FCLD).
  * `classes.py`: Estruturas de dados para Ve√≠culos e Carregadores.

-----

## üìä Resultados ap√≥s 9.773 Itera√ß√µes

O modelo foi treinado por quase 10.000 ciclos de simula√ß√£o ("dias"). Abaixo est√£o os resultados obtidos, comprovando a efic√°cia da pol√≠tica ADP.

![Painel comparativo entre ADP e EDLD](fig_dashboard.png)

### 1. Intelig√™ncia de Mercado (Load Shifting)

O gr√°fico abaixo √© a prova principal da intelig√™ncia do sistema.

  * **Linha Vermelha:** Pre√ßo da energia (Pico entre 17h e 20h).
  * **Barras Cinzas (Regra Fixa - EDLD):** Carregam assim que o ve√≠culo chega, pagando o pre√ßo m√°ximo.
  * **Barras Azuis (IA - ADP):** O modelo **interrompe o carregamento** durante o hor√°rio caro e retoma massivamente assim que o pre√ßo cai (ap√≥s as 20h).

> *Fig 9: Demonstra√ß√£o do "Load Shifting". A IA evita completamente a zona de pre√ßo alto.*

![Perfil de carga di√°rio comparando ADP e EDLD](fig9_perfil_carga.png)

### 2. Efici√™ncia Operacional (N√≠vel de Servi√ßo)

Apesar de economizar dinheiro "esperando" o pre√ßo baixar, o modelo n√£o falha na entrega.

  * A **meta de 100%** (linha tracejada) foi atingida.
  * Isso foi garantido atrav√©s de uma fun√ß√£o de penalidade calibrada (`PENALTY = 200.0`), ensinando √† IA que "n√£o carregar custa mais caro que a energia".

> *Comparativo: O ADP atinge o mesmo n√≠vel de servi√ßo (100%) que as regras tradicionais, mas com intelig√™ncia de custo.*

### 3. Converg√™ncia do Aprendizado

O gr√°fico mostra a evolu√ß√£o dos pesos ($\zeta$) que comp√µem a "intui√ß√£o" da IA ao longo das 9.773 itera√ß√µes.

  * Os **picos** representam momentos onde o modelo encontrou cen√°rios raros/dif√≠ceis e precisou ajustar drasticamente sua estrat√©gia.
  * A estabilidade entre os picos indica que uma pol√≠tica robusta foi encontrada.

> *Evolu√ß√£o dos coeficientes da Fun√ß√£o de Valor Aproximada ao longo de 9774 itera√ß√µes.*

![Curvas de converg√™ncia dos pesos ao longo do treinamento](fig3_convergencia.png)

-----

## üìä Resultados ap√≥s 50.198 Itera√ß√µes (‚âà12 horas)

Executamos um treinamento cont√≠nuo por **50.198 itera√ß√µes** (aprox. 12 horas de simula√ß√£o). O sistema manteve estabilidade num√©rica e continuou a refinar o deslocamento de carga, gerando os panoramas abaixo.

![Perfil de carga di√°rio comparando ADP e EDLD (50k itera√ß√µes)](fig9_perfil_carga_50k.png)

![Curvas de converg√™ncia dos pesos ao longo de 50k itera√ß√µes](fig3_convergencia_50k.png)

**Conclus√µes principais:**

- A pol√≠tica ADP continua concentrando a maior parte da energia no per√≠odo p√≥s-20h, minimizando custos em rela√ß√£o ao EDLD mesmo com cen√°rios extremos simulados.
- O n√≠vel de servi√ßo permaneceu pr√≥ximo de 100%, indicando que a pol√≠tica explorada mant√©m a confiabilidade ap√≥s longas sess√µes de treinamento.
- Os pesos ($\zeta$) estabilizam em torno de uma faixa estreita ap√≥s cerca de 30k itera√ß√µes, oscilando pontualmente apenas quando surgem dias com penalidades altas ‚Äî comportamento esperado para um ADP com regress√£o incremental.

-----

## üöÄ Como Rodar o Projeto

### Pr√©-requisitos

Voc√™ precisar√° de Python 3.x e do solver GLPK instalado no sistema.

```bash
# 1. Instalar depend√™ncias Python
pip install numpy matplotlib scikit-learn pyomo

# 2. Instalar Solver GLPK (Linux/Ubuntu)
sudo apt-get install glpk-utils

# 2. Instalar Solver GLPK (MacOS)
brew install glpk
```

### Execu√ß√£o

Para iniciar (ou continuar) o treinamento:

```bash
python main.py
```

O sistema possui um menu interativo:

  * O script salva automaticamente um `adp_checkpoint.json`.
  * Ao reiniciar, ele perguntar√° se deseja **[C]ontinuar** o treino anterior ou **[R]einiciar**.
  * Pressione `Ctrl+C` a qualquer momento para pausar e gerar os gr√°ficos finais.

-----

## üìö Refer√™ncia Cient√≠fica

Este c√≥digo √© uma implementa√ß√£o baseada no trabalho:

> **Mahyari, E., & Freeman, N. (2025).** Electric vehicle fleet charging management: An approximate dynamic programming policy. *European Journal of Operational Research*, 327, 263-279.

-----

*Implementado por Jess√©, Novembro de 2025.*
